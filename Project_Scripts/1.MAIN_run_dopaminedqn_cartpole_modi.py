# -*- coding: utf-8 -*-
"""1.MAIN_run_dopaminedqn_cartpole_modi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jp_ASkbZoWfFnwUBmxxELa6UOQGJbinD
"""

#!pip uninstall absl-py
#!pip uninstall pyglet

'''
!pip install pyglet~=1.3.2
!apt install -y graphviz
!pip install flax
!pip install graphviz
!pip install pyvirtualdisplay
!apt-get install python-opengl -y
!apt install xvfb -y
!pip install 'gym[atari]'
!pip install -U dopamine-rl
'''

import flax
from graphviz import Digraph
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt
import numpy as onp

from IPython.display import HTML
from pprint import pprint
import logging
from pyvirtualdisplay import Display
logging.getLogger("pyvirtualdisplay").setLevel(logging.ERROR)

#display = Display(visible=0, size=(1024, 768))
#display.start()
import os
#os.environ["DISPLAY"] = ":" + str(display.display)

import gym
cartpole_env = gym.make('CartPole-v1')
cartpole_env.reset()

from google.colab import drive 
drive.mount('/content/drive')
path = '/content/drive/My Drive/SaveFiles/Data/'

import sys
sys.path.append(path)

from networks_new import *

#from dopamine.jax import networks
#from dopamine.jax.agents.dqn import dqn_agent

from dqn_agent_new import *
import gin

cartpole_config = """
JaxDQNAgent.gamma = 0.99
JaxDQNAgent.update_horizon = 1
JaxDQNAgent.min_replay_history = 500
JaxDQNAgent.update_period = 4
JaxDQNAgent.target_update_period = 100
JaxDQNAgent.epsilon_fn = @dqn_agent.identity_epsilon

create_optimizer.name = 'adam'
create_optimizer.learning_rate = 0.001
create_optimizer.eps = 3.125e-4

OutOfGraphReplayBuffer.replay_capacity = 50000
OutOfGraphReplayBuffer.batch_size = 128
"""

#Configs
#DQN => double_dqn = False & network=CartpoleDQNNetwork
#DoubleDQN(DDQN) => double_dqn =True & network=CartpoleDQNNetwork
#Dueling DQN(DuelingDQN) => double_dqn = False & network=CartpoleDuelingDQNNetwork
#Dueling DDQN => double_dqn = True & network=CartpoleDuelingDQNNetwork
#ms_los function -> mse_loss = True

gin.parse_config(cartpole_config, skip_unknown=False)
dqn_agent = JaxDQNAgent(num_actions=cartpole_env.action_space.n,
                                  observation_shape=(4, 1),
                                  observation_dtype=jnp.float64,
                                  stack_size=1,
                                  network=CartpoleDuelingDQNNetwork, 
                                  double_dqn = True,
                                  mse_inf = False)

def learned_policy(s):
  return dqn_agent.step(0., s)  # We pass in a dummy reward

# We set our agent in `eval_mode` to avoid it from continuing to train while
# interacting with the environment.
dqn_agent.eval_mode = True
#animate_agent(learned_policy, cartpole_env, num_frames=100)

max_steps_per_episode = 200  # @param {type:'slider', min:10, max:1000}
training_steps = 1000  # @param {type:'slider', min:10, max:5000}
num_iterations = 30  # @param {type:'slider', min:10, max:200}

# First remove eval mode!
dqn_agent.eval_mode = False
average_returns = []
# Each iteration will consist of a number of episodes.
for iteration in range(num_iterations):
  step_count = 0
  num_episodes = 0
  sum_returns = 0.
  # This while loop will continue running episodes until we've done enough
  # training steps.
  while step_count < training_steps:
    episode_length = 0
    episode_rewards = 0.
    s = cartpole_env.reset()
    a = dqn_agent.begin_episode(s)
    is_terminal = False
    # Run the episode until termination.
    while True:
      s, r, done, _ = cartpole_env.step(a)
      episode_rewards += r
      episode_length += 1
      if done or episode_length == max_steps_per_episode:
        # Stop the loop if the episode or ended or we've reached the max steps.
        break
      else:
        a = dqn_agent.step(r, s)
    dqn_agent.end_episode(r)
    step_count += episode_length
    sum_returns += episode_rewards
    num_episodes += 1
  average_return = sum_returns / num_episodes if num_episodes > 0 else 0.
  print(f'Iteration {iteration}: average return = {average_return:.4f}')
  average_returns.append(average_return)

import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))
plt.plot(average_returns)
plt.title('Dueling DQN Training')
plt.xlabel('# of episodes')
plt.ylabel('score')
plt.show()