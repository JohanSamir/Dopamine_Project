# Hyperparameters follow Dabney et al. (2017) but we modify as necessary to
# match those used in Rainbow (Hessel et al., 2018), to ensure apples-to-apples
# comparison.

import dopamine.jax.agents.rainbow.rainbow_agent
import dopamine.jax.agents.dqn.dqn_agent

import dopamine.discrete_domains.gym_lib
import dopamine.discrete_domains.run_experiment
import networks_new

#import dopamine.jax.agents.quantile.quantile_agent
import dopamine.discrete_domains.atari_lib
import dopamine.discrete_domains.run_experiment
import dopamine.replay_memory.prioritized_replay_buffer

JaxxQuantileAgent.observation_shape = (8,1)
JaxxQuantileAgent.observation_dtype = %jax_networks.LUNALANDER_OBSERVATION_DTYPE
JaxxQuantileAgent.stack_size = 1
JaxxQuantileAgent.network  = @networks_new.QuantileNetwork 

JaxxQuantileAgent.kappa = 1.0
JaxxQuantileAgent.num_atoms = 51
JaxxQuantileAgent.gamma = 0.99
JaxxQuantileAgent.update_horizon = 3
JaxxQuantileAgent.min_replay_history = 500 # agent steps
JaxxQuantileAgent.update_period = 4
JaxxQuantileAgent.target_update_period = 300 #100[OK] agent steps

JaxxQuantileAgent.minatar = False
JaxxQuantileAgent.env = None
JaxxQuantileAgent.normalize_obs = False
JaxxQuantileAgent.hidden_layer = 2
JaxxQuantileAgent.neurons = 512

JaxxQuantileAgent.double_dqn = False
JaxxQuantileAgent.noisy = False
JaxxQuantileAgent.dueling = True

JaxxQuantileAgent.replay_scheme = 'prioritized'
JaxxQuantileAgent.optimizer = 'adam'

create_optimizer.learning_rate = 1e-3
create_optimizer.eps = 3.125e-4

create_gym_environment.environment_name = 'LunarLander'
create_gym_environment.version = 'v2'


JaxxQuantileAgent.epsilon_fn = @dqn_agent.identity_epsilon

TrainRunner.create_environment_fn = @gym_lib.create_gym_environment
Runner.num_iterations = 30
Runner.training_steps = 4000
Runner.max_steps_per_episode = 1000

OutOfGraphPrioritizedReplayBuffer.replay_capacity = 50000
OutOfGraphPrioritizedReplayBuffer.batch_size = 128
