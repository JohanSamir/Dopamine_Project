# Hyperparameters follow Dabney et al. (2017) but we modify as necessary to
# match those used in Rainbow (Hessel et al., 2018), to ensure apples-to-apples
# comparison.

import dopamine.jax.agents.rainbow.rainbow_agent
import dopamine.jax.agents.dqn.dqn_agent

import dopamine.discrete_domains.gym_lib
import dopamine.discrete_domains.run_experiment
import networks_new

#import dopamine.jax.agents.quantile.quantile_agent
import dopamine.discrete_domains.atari_lib
import dopamine.discrete_domains.run_experiment
import dopamine.replay_memory.prioritized_replay_buffer

JaxxQuantileAgent.observation_shape = %gym_lib.CARTPOLE_OBSERVATION_SHAPE
JaxxQuantileAgent.observation_dtype = %jax_networks.CARTPOLE_OBSERVATION_DTYPE
JaxxQuantileAgent.stack_size = %gym_lib.CARTPOLE_STACK_SIZE
JaxxQuantileAgent.network  = @networks_new.QuantileNetwork 

JaxxQuantileAgent.kappa = 1.0
JaxxQuantileAgent.num_atoms = 51
JaxxQuantileAgent.gamma = 0.99
JaxxQuantileAgent.update_horizon = 3
JaxxQuantileAgent.min_replay_history = 500 # agent steps
JaxxQuantileAgent.update_period = 2
JaxxQuantileAgent.target_update_period = 100 # agent steps

JaxxQuantileAgent.minatar = False
JaxxQuantileAgent.env = "CartPole"
JaxxQuantileAgent.normalize_obs = True
JaxxQuantileAgent.hidden_layer = 2
JaxxQuantileAgent.neurons = 512

JaxxQuantileAgent.double_dqn = True
JaxxQuantileAgent.noisy = True
JaxxQuantileAgent.dueling = False

JaxxQuantileAgent.replay_scheme = 'prioritized'
JaxxQuantileAgent.optimizer = 'adam'

create_optimizer.learning_rate = 0.0001
create_optimizer.eps = 0.0003125

create_gym_environment.environment_name = 'CartPole'
create_gym_environment.version = 'v0'

JaxxQuantileAgent.epsilon_fn = @dqn_agent.identity_epsilon

TrainRunner.create_environment_fn = @gym_lib.create_gym_environment
Runner.num_iterations = 30
Runner.training_steps = 1000
Runner.max_steps_per_episode = 200

OutOfGraphPrioritizedReplayBuffer.replay_capacity = 50000
OutOfGraphPrioritizedReplayBuffer.batch_size = 128
