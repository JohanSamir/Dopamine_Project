# Hyperparameters follow Dabney et al. (2017) but we modify as necessary to
# match those used in Rainbow (Hessel et al., 2018), to ensure apples-to-apples
# comparison.

import dopamine.jax.agents.rainbow.rainbow_agent
import dopamine.jax.agents.dqn.dqn_agent

import dopamine.discrete_domains.gym_lib
import dopamine.discrete_domains.run_experiment
import networks_new

#import dopamine.jax.agents.quantile.quantile_agent
import dopamine.discrete_domains.atari_lib
import dopamine.discrete_domains.run_experiment
import dopamine.replay_memory.prioritized_replay_buffer

JaxQuantileAgentNew.observation_shape = %gym_lib.CARTPOLE_OBSERVATION_SHAPE
JaxQuantileAgentNew.observation_dtype = %jax_networks.CARTPOLE_OBSERVATION_DTYPE
JaxQuantileAgentNew.stack_size = %gym_lib.CARTPOLE_STACK_SIZE
JaxQuantileAgentNew.network  = @networks_new.QuantileNetwork 

JaxQuantileAgentNew.kappa = 1.0
JaxQuantileAgentNew.num_atoms = 51
JaxQuantileAgentNew.gamma = 0.99
JaxQuantileAgentNew.update_horizon = 3
JaxQuantileAgentNew.min_replay_history = 500 # agent steps
JaxQuantileAgentNew.update_period = 2
JaxQuantileAgentNew.target_update_period = 100 # agent steps

JaxQuantileAgentNew.net_conf = 'classic'
JaxQuantileAgentNew.env = 'CartPole'
JaxQuantileAgentNew.normalize_obs = True
JaxQuantileAgentNew.hidden_layer = 2
JaxQuantileAgentNew.neurons = 512

JaxQuantileAgentNew.double_dqn = False
JaxQuantileAgentNew.noisy = False
JaxQuantileAgentNew.dueling = False

JaxQuantileAgentNew.replay_scheme = 'prioritized'
JaxQuantileAgentNew.optimizer = 'adam'

create_optimizer.learning_rate = 0.0001
create_optimizer.eps = 0.0003125

create_gym_environment.environment_name = 'CartPole'
create_gym_environment.version = 'v0'

JaxQuantileAgentNew.epsilon_fn = @dqn_agent.identity_epsilon

TrainRunner.create_environment_fn = @gym_lib.create_gym_environment
Runner.num_iterations = 30
Runner.training_steps = 1000
Runner.max_steps_per_episode = 200

OutOfGraphPrioritizedReplayBuffer.replay_capacity = 50000
OutOfGraphPrioritizedReplayBuffer.batch_size = 128
